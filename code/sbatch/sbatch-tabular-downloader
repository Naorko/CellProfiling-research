#!/bin/bash

##################
### sbatch configuration parameters must start with #SBATCH and must precede any other commands.
### To ignore, just add another # - like ##SBATCH
##################

#SBATCH --partition main			### specify partition name where to run a job. debug: 2 hours limit; short: 7 days limit; gtx1080: 7 days
#SBATCH --time 5-12:00:00			### limit the time of job running, partition limit can override this. Format: D-H:MM:SS
#SBATCH --job-name CellProfile	### name of the job
#SBATCH --output job_outputs/job-metadata-downloader-%J.out			### output log for running job - %J for job number
#SBATCH --mail-user=naorko@post.bgu.ac.il	### user email for sending job status
#SBATCH --mail-type=END			### conditions when to send the email. ALL,BEGIN,END,FAIL, REQUEU, NONE

##SBATCH --gres=gpu:1				### number of GPUs, ask for more than 1 only if you can parallelize your code for multi GPU
#SBATCH --mem=32G				### ammount of RAM memory
#SBATCH --cpus-per-task=4			### number of CPU cores

### Print some data to output file ###
echo `date`echo -e "\nSLURM_JOBID:\t\t" $SLURM_JOBID
echo -e "SLURM_JOB_NODELIST:\t" $SLURM_JOB_NODELIST "\n\n"

### Start you code below ####
module load anaconda				### load anaconda module (must present when working with conda environments)
source activate tf-env				### activating environment, environment must be configured before running the job
python $CELL_STORAGE/CellProfiling-research/code/plateDownloader.py $CELL_STORAGE/plates -l 24294 24509 24619 24687 24792 25664 25912 25987 26577 26666 24300 24517 24633 24735 25422 25676 25937 25997 26622 26674 24303 24562 24640 24752 25492 25680 25938 26562 26640 26765 24311 24594 24654 24773 25571 25686 25945 26563 26641 26771 24321 24617 24661 24774 25575 25708 25985 26576 26662 26786