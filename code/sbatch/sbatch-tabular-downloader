#!/bin/bash

##################
### sbatch configuration parameters must start with #SBATCH and must precede any other commands.
### To ignore, just add another # - like ##SBATCH
##################

#SBATCH --partition main			### specify partition name where to run a job. debug: 2 hours limit; short: 7 days limit; gtx1080: 7 days
#SBATCH --time 7-00:00:00			### limit the time of job running, partition limit can override this. Format: D-H:MM:SS
#SBATCH --job-name CellProfile	### name of the job
#SBATCH --output job_outputs/job-tabular-downloader-%J.out			### output log for running job - %J for job number
#SBATCH --mail-user=naorko@post.bgu.ac.il	### user email for sending job status
#SBATCH --mail-type=END			### conditions when to send the email. ALL,BEGIN,END,FAIL, REQUEU, NONE

##SBATCH --gres=gpu:1				### number of GPUs, ask for more than 1 only if you can parallelize your code for multi GPU
#SBATCH --mem=32G				### ammount of RAM memory
#SBATCH --cpus-per-task=4			### number of CPU cores

### Print some data to output file ###
echo `date`echo -e "\nSLURM_JOBID:\t\t" $SLURM_JOBID
echo -e "SLURM_JOB_NODELIST:\t" $SLURM_JOB_NODELIST "\n\n"

### Start you code below ####
module load anaconda				### load anaconda module (must present when working with conda environments)
source activate tf-env				### activating environment, environment must be configured before running the job
python $CELL_HOME/code/plateDownloader.py $CELL_STORAGE/plates -l 24684 25690 25704 26166 24313 24306 25983 24278 24732 24563
##26669 26588 26601 25966 24655 26739 26772 26126 24304 24566 <-- 847100
##24602 24793 24635 26702 24772 25935 24352 24667 24646 24277
##24604 25939 25592 25915 25678 26569 26668 25856 24564 25568
##24605 25848 25925 25605 25679 26572 26081 24657 25403 24648
##24684 25690 25704 26166 24313 24306 25983 24278 24732 24563 <-- 847104