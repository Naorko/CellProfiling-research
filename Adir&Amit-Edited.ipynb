{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are importing all relevant packages for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T09:42:39.510247Z",
     "start_time": "2020-08-26T09:42:30.284358Z"
    }
   },
   "outputs": [],
   "source": [
    "# connections and OS\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import os\n",
    "import sqlite3\n",
    "#import csv\n",
    "\n",
    "#utils (Pandas,numpy,tqdm)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "#visualize \n",
    "#import seaborn as sns\n",
    "\n",
    "#preprocessing, metrices and splits \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "#ML models:\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "\n",
    "#tensorflow layer, callbacks and layers\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change directory to project directory within the department cluster (SLURM) and define to constant variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T09:42:49.843122Z",
     "start_time": "2020-08-26T09:42:49.836703Z"
    }
   },
   "outputs": [],
   "source": [
    "#change directory to project directory within the department cluster (SLURM)\n",
    "PROJECT_DIRECTORY = r'C:\\Users\\Niko\\Desktop\\plates'\n",
    "\n",
    "CHANNELS = [\"AGP\",\"DNA\",\"ER\",\"Mito\",\"RNA\"]\n",
    "LABEL_FIELD = 'Metadata_ASSAY_WELL_ROLE'\n",
    "S_STD = 'Std'\n",
    "S_MinMax = 'MinMax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PROJECT_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T09:42:50.672093Z",
     "start_time": "2020-08-26T09:42:50.661259Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_scaler(df, scale_method):\n",
    "    \"\"\"\n",
    "    This function is fitting a scaler using one of two methods: STD and MinMax\n",
    "    df: dataFrame to fit on\n",
    "    scale_method: 'Std' or 'MinMax'\n",
    "    return: scaled dataframe, according to StandardScaler or according to MinMaxScaler\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if scale_method == S_STD:\n",
    "        scaler = StandardScaler()\n",
    "    elif scale_method == S_MinMax:\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    if not scaler:\n",
    "        return None\n",
    "\n",
    "    return scaler.fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def scale_data(df, scaler):\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
    "    scaled_df.fillna(0,inplace=True)\n",
    "    return scaled_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:02:32.110621Z",
     "start_time": "2020-08-26T10:02:32.088820Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_channels_x_and_y(filename, task_channel):\n",
    "    \"\"\"\n",
    "    This function is responsible for splitting five channels into four channels as train and the remaining channel to test\n",
    "    filename: file path to the cell table from a single plate\n",
    "    task_channel: the current channel that we aim to predict\n",
    "    \n",
    "    Notably: In order to avoid leakage we drop all 'correlation features\n",
    "    return: separated dataframes x_features and y_df.\n",
    "            x_features: contains all available features excluding the features related to 'task_channel' we aim to predict\n",
    "            y_df: contains all available features related to 'task_channel' only\n",
    "    \"\"\"\n",
    "\n",
    "    # Data preparation\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.set_index([LABEL_FIELD, 'Metadata_broad_sample', 'ImageNumber', 'ObjectNumber'])\n",
    "    df.drop(['TableNumber'], inplace=True, axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    labels_not_indexed = ['Image_Metadata_Well']\n",
    "    general_cols = [f for f in df.columns if f not in labels_not_indexed and all(c not in f for c in CHANNELS)]\n",
    "    corr_cols = [f for f in df.columns if 'Correlation' in f]\n",
    "\n",
    "    # Split columns by channel\n",
    "    dict_channel_cols = {}\n",
    "    for channel in CHANNELS:\n",
    "        dict_channel_cols[channel] = [col for col in df.columns if channel in col and col not in corr_cols]\n",
    "\n",
    "    not_curr_channel_cols = [col for channel in CHANNELS if channel != task_channel\n",
    "                             for col in dict_channel_cols[channel]]\n",
    "    cols = general_cols + not_curr_channel_cols\n",
    "\n",
    "    x_features_df = df[cols]\n",
    "\n",
    "    y_df = df[dict_channel_cols[task_channel]]\n",
    "\n",
    "    return x_features_df, y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following three cells we are creating three ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:13:53.617800Z",
     "start_time": "2020-08-26T10:13:53.610909Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_LR(df_train_X, df_train_Y):  \n",
    "    \"\"\"\n",
    "    In this cell we are creating and training a linear regression model        \n",
    "    df_train_X: contains all available features excluding the features related to 'task_channel' we aim to predict (train)\n",
    "    df_train_Y: contains all available features related to 'task_channel' only for the train\n",
    "    \n",
    "    \n",
    "    return: trained linear regression model\n",
    "    \"\"\"\n",
    "    LR_model = LinearRegression()    \n",
    "    LR_model.fit(df_train_X.values,df_train_Y.values)\n",
    "    return LR_model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:13:53.890850Z",
     "start_time": "2020-08-26T10:13:53.884664Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_Ridge(df_train_X, df_train_Y):\n",
    "    \"\"\"    \n",
    "    In this cell we are creating and training a ridge regression model    \n",
    "    \n",
    "    \n",
    "    df_train_X: contains all available features excluding the features related to 'task_channel' we aim to predict (train)\n",
    "    df_train_Y: contains all available features related to 'task_channel' only for the train    \n",
    "    \n",
    "    return: trained ridge regression model\n",
    "    \"\"\"\n",
    "    Ridge_model = Ridge()    \n",
    "    Ridge_model.fit(X=df_train_X.values,y=df_train_Y.values)    \n",
    "    return Ridge_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:13:54.196170Z",
     "start_time": "2020-08-26T10:13:54.176523Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_model_dnn(task_channel,df_train_X, df_train_Y,test_plate):\n",
    "    \"\"\"    \n",
    "    In this cell we are creating and training a multi layer perceptron (we refer to it as deep neural network, DNN) model\n",
    "    \n",
    "    task_channel: the current channel that we aim to predict\n",
    "    df_train_X: contains all available features excluding the features related to 'task_channel' we aim to predict (train)\n",
    "    df_train_Y: contains all available features related to 'task_channel' only for the train\n",
    "    test_plate: the ID of a given plate. This information assist us while printing the results.\n",
    "    \n",
    "    return: trained dnn model\n",
    "    \"\"\"\n",
    "    # Stracture of the network#\n",
    "    inputs = Input(shape=(df_train_X.shape[1],))\n",
    "    dense1 = Dense(512,activation = 'relu')(inputs)\n",
    "    dense2 = Dense(256,activation = 'relu')(dense1)\n",
    "    dense3 = Dense(128,activation = 'relu')(dense2)    \n",
    "    dense4 = Dense(100,activation = 'relu')(dense3)\n",
    "    dense5 = Dense(50,activation = 'relu')(dense4)\n",
    "    dense6 = Dense(25,activation = 'relu')(dense5)\n",
    "    dense7 = Dense(10,activation = 'relu')(dense6)\n",
    "    predictions = Dense(df_train_Y.shape[1],activation='sigmoid')(dense7)\n",
    "    \n",
    "    #model compiliation\n",
    "    model = Model(inputs=inputs,outputs = predictions)\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    \n",
    "    #model training    \n",
    "    test_plate_number = test_plate[:5]\n",
    "    folder = os.path.join(PROJECT_DIRECTORY, 'Models')\n",
    "    filepath = os.path.join(folder, f'{test_plate_number}_{task_channel}.h5')\n",
    "    my_callbacks = [ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)]\n",
    "    model.fit(df_train_X,df_train_Y,epochs = 5,batch_size=1024*8,verbose=0,shuffle=True,validation_split=0.2,callbacks=my_callbacks)\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:13:54.704037Z",
     "start_time": "2020-08-26T10:13:54.691051Z"
    }
   },
   "outputs": [],
   "source": [
    "#    print_results(test_plate_number, task_channel, \"Overall\", \"DNN\", \"None\", \"MSE\", str(mean_squared_error(model_pred,channel_task_y)))\n",
    "def print_results(plate_number, channel, family, model, _type, metric, value):\n",
    "    \"\"\"\n",
    "    This function is creating a csv named: 'results' that contains all of the models’ performance (e.g. MSE) for each plate and each family of attributes\n",
    "    plate_number: ID of palte\n",
    "    channel: The channel we aim to predict\n",
    "    family: features united by their charactheristics (e.g., Granularity, Texture)\n",
    "    model: the model name\n",
    "    _type: scaling method (e.g., MinMax Scaler or StandardScaler)\n",
    "    metric: MSE/MAE\n",
    "    value: value of the metric error    \n",
    "    \"\"\"\n",
    "    results_path = os.path.join(PROJECT_DIRECTORY, 'Results')\n",
    "    file_path = os.path.join(results_path, 'results.csv')\n",
    "    files_list = os.listdir(results_path)\n",
    "    if 'results.csv' not in files_list:\n",
    "        file1 = open(file_path,\"a+\")     \n",
    "        file1.write(\"Plate,Channel,Family,Model,Type,Metric,Value \\n\")\n",
    "        file1.write(plate_number+\",\"+channel+\",\"+family+\",\"+model+\",\"+_type+\",\"+metric+\",\"+value+\"\\n\")\n",
    "        file1.close()\n",
    "    else:\n",
    "        file1 = open(file_path, \"a+\")\n",
    "        file1.write(plate_number+\",\"+channel+\",\"+family+\",\"+model+\",\"+_type+\",\"+metric+\",\"+value+\"\\n\")\n",
    "        file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_family_MSE(test_plate_number, task_channel, model, _type, df, channel_task_y):    \n",
    "    \"\"\"\n",
    "    This function is calculating the MSE/MAE measures for plates based on different models\n",
    "    test_plate_number: ID of the examine plate\n",
    "    task_channel: Channel we aim to predict\n",
    "    model: model name\n",
    "    _type: scaling method (e.g., MinMax Scaler or StandardScaler)\n",
    "    df: prediction of any given ML model which aim to predict the channel_task_y\n",
    "    channel_task_y: features corresponding to the 'task channel' (channel we aim to predict)    \n",
    "    \"\"\"\n",
    "    Families = {'Granularity':[],\n",
    "               'Intensity':[],\n",
    "               'Location':[],\n",
    "               'RadialDistribution':[],\n",
    "               'Texture':[]}\n",
    "\n",
    "    for name in channel_task_y.columns:\n",
    "        if '_Granularity' in name:\n",
    "            Families['Granularity'].append(name)\n",
    "        elif '_Intensity' in name:\n",
    "            Families['Intensity'].append(name)\n",
    "        elif '_Location' in name:\n",
    "            Families['Location'].append(name)        \n",
    "        elif '_RadialDistribution' in name:\n",
    "            Families['RadialDistribution'].append(name)\n",
    "        elif '_Texture' in name:\n",
    "            Families['Texture'].append(name)\n",
    "            \n",
    "    for key in Families.keys():\n",
    "        try:            \n",
    "            print_results(test_plate_number, task_channel, key, model, _type, \"MSE\", str(mean_squared_error(df[Families[key]],channel_task_y[Families[key]])))\n",
    "        except:\n",
    "            if len(Families[key]) == 0:\n",
    "                print('empty family {}'.format(key))\n",
    "            else:\n",
    "                print('problem in mse key')\n",
    "            \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_family_MAE(test_plate_number, task_channel, model, _type, df, channel_task_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is calculating the MSE/MAE measures for plates based on different models\n",
    "    test_plate_number: ID of the examine plate\n",
    "    task_channel: Channel we aim to predict\n",
    "    model: model name\n",
    "    _type: scaling method (e.g., MinMax Scaler or StandardScaler)\n",
    "    df: prediction of any given ML model which aim to predict the channel_task_y\n",
    "    channel_task_y: features corresponding to the 'task channel' (channel we aim to predict)    \n",
    "    \"\"\"\n",
    "    \n",
    "    Families = {'Granularity':[],\n",
    "               'Intensity':[],\n",
    "               'Location':[],\n",
    "               'RadialDistribution':[],\n",
    "               'Texture':[]}\n",
    "\n",
    "    for name in channel_task_y.columns:\n",
    "        if '_Granularity' in name:\n",
    "            Families['Granularity'].append(name)\n",
    "        elif '_Intensity' in name:\n",
    "            Families['Intensity'].append(name)\n",
    "        elif '_Location' in name:\n",
    "            Families['Location'].append(name)        \n",
    "        elif '_RadialDistribution' in name:\n",
    "            Families['RadialDistribution'].append(name)\n",
    "        elif '_Texture' in name:\n",
    "            Families['Texture'].append(name)\n",
    "            \n",
    "    for key in Families.keys():\n",
    "        try:            \n",
    "            print_results(test_plate_number, task_channel, key, model, _type, \"MAE\", str(mean_absolute_error(df[Families[key]],channel_task_y[Families[key]])))\n",
    "        except:\n",
    "            if len(Families[key]) == 0:\n",
    "                print('empty family {}'.format(key))\n",
    "            else:\n",
    "                print('problem in mae key')\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def main(path, scale_method):\n",
    "    \"\"\"\n",
    "    This is the main function of the preprocessing steps.\n",
    "    This function will iterate all over the sqlite files and do the following:\n",
    "    1) prepate train + test files\n",
    "    2) scale train + test files (x + y values separately)\n",
    "    3) return: \n",
    "        task_channel -> string, reflect the relevant channel for test. For example, 'AGP'\n",
    "        df_train_X -> DataFrame, (instances,features) for the train set\n",
    "        df_train_Y -> DataFrame, (instances,labels) for the train set\n",
    "        channel_task_x -> DataFrame, (instances,features) for the test set\n",
    "        channel_task_y -> DataFrame, (instances,labels) for the test set\n",
    "    \"\"\"\n",
    "\n",
    "    csv_files= [_ for _ in os.listdir(path) if _.endswith(\".csv\")]\n",
    "    for task_channel in tqdm(CHANNELS):        \n",
    "        # This is the current file that we will predict        \n",
    "        for test_plate in csv_files:\n",
    "            print(test_plate)\n",
    "\n",
    "            channel_task_x, channel_task_y = split_channels_x_and_y(path + test_plate, task_channel)\n",
    "            print(channel_task_x.index.unique(0).tolist())\n",
    "            print(channel_task_x.index.unique(1).tolist())\n",
    "\n",
    "            channel_task_x_mock = channel_task_x[channel_task_x.index.isin(['mock'], 0)]\n",
    "            channel_task_x_treated = channel_task_x[channel_task_x.index.isin(['treated'], 0)]\n",
    "\n",
    "            channel_task_y_mock = channel_task_y.loc[channel_task_x_mock.index]\n",
    "            channel_task_y_treated = channel_task_y.loc[channel_task_x_treated.index]\n",
    "\n",
    "            # Extracting all train samples from other files - only mock\n",
    "            list_x_df = []\n",
    "            list_y_df = []\n",
    "            for train_plate in tqdm(csv_files):\n",
    "                if train_plate!=test_plate:\n",
    "                    curr_x, curr_y = split_channels_x_and_y(path + train_plate, task_channel)\n",
    "                    curr_x = curr_x[curr_x.index.isin(['mock'], 0)]\n",
    "                    curr_y = curr_y.loc[curr_x.index]\n",
    "\n",
    "                    list_x_df.append(curr_x)\n",
    "                    list_y_df.append(curr_y)\n",
    "\n",
    "            df_train_x = pd.concat(list_x_df)\n",
    "            df_train_y = pd.concat(list_y_df)\n",
    "\n",
    "            # Scale for training set#\n",
    "            x_scaler = fit_scaler(df_train_x, scale_method)\n",
    "            y_scaler = fit_scaler(df_train_y, scale_method)\n",
    "\n",
    "            df_train_x_scaled = scale_data(df_train_x, x_scaler)\n",
    "            df_train_y_scaled = scale_data(df_train_y, y_scaler)\n",
    "\n",
    "            df_test_treated_x_scaled = scale_data(channel_task_x_treated, x_scaler)\n",
    "            df_test_treated_y_scaled = scale_data(channel_task_y_treated, y_scaler)\n",
    "            df_test_mock_x_scaled = scale_data(channel_task_x_mock, x_scaler)\n",
    "            df_test_mock_y_scaled = scale_data(channel_task_y_mock, y_scaler)\n",
    "\n",
    "            # #### Output\n",
    "            # os.makedirs('DATA', exist_ok=True)\n",
    "            # plate_number = test_plate.split('.')[0]\n",
    "            # df_train_x.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_train_x.csv'))\n",
    "            # df_train_y.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_train_y.csv'))\n",
    "            # channel_task_x_mock.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_test_x_mock.csv'))\n",
    "            # channel_task_x_treated.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_test_x_treated.csv'))\n",
    "            # channel_task_y_mock.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_test_y_mock.csv'))\n",
    "            # channel_task_y_treated.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_test_y_treated.csv'))\n",
    "            #\n",
    "            # df_train_x_scaled.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_train_x_scaled.csv'))\n",
    "            # df_train_y_scaled.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_train_y_scaled.csv'))\n",
    "            # df_test_mock_x_scaled.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_test_x_mock_scaled.csv'))\n",
    "            # df_test_treated_x_scaled.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_test_x_treated_scaled.csv'))\n",
    "            # df_test_mock_y_scaled.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_test_y_mock_scaled.csv'))\n",
    "            # df_test_treated_y_scaled.to_csv(os.path.join('Data', f'{plate_number}_{task_channel}_test_y_treated_scaled.csv'))\n",
    "            #\n",
    "            # ####\n",
    "            \n",
    "\n",
    "            # Model Creation - AVG MSE for each model:\n",
    "            print(test_plate)\n",
    "            print(task_channel+\":\")\n",
    "            LR_model = create_LR(df_train_x_scaled, df_train_y_scaled)\n",
    "            Ridge_model = create_Ridge(df_train_x_scaled, df_train_y_scaled)\n",
    "            DNN_model = create_model_dnn(task_channel,df_train_x_scaled, df_train_y_scaled,test_plate)\n",
    "#             svr_model = create_SVR(task_channel,df_train_x_scaled, df_train_y_scaled, channel_task_x, channel_task_y)\n",
    "            #return task_channel,df_train_X, df_train_Y, channel_task_x, channel_task_y\n",
    "    \n",
    "            print('**************')\n",
    "            print('LR')\n",
    "            print('profile_treated:') \n",
    "            yhat_lr = pd.DataFrame(LR_model.predict(df_test_treated_x_scaled.values), columns=df_test_treated_y_scaled.columns)\n",
    "            print('Linear Reg MSE: {}'.format(mean_squared_error(yhat_lr, df_test_treated_y_scaled.values)))\n",
    "            print('Linear Reg MAE: {}'.format(mean_absolute_error(yhat_lr, df_test_treated_y_scaled.values)))\n",
    "            \n",
    "            print_results(test_plate, task_channel, 'Overall', 'Linear Regression', 'Treated', 'MSE', str(mean_squared_error(yhat_lr,df_test_treated_y_scaled.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'Linear Regression', 'Treated', 'MAE', str(mean_absolute_error(yhat_lr,df_test_treated_y_scaled.values)))\n",
    "            \n",
    "            get_family_MSE(test_plate, task_channel, \"Linear Regression\", \"Treated\", yhat_lr, df_test_treated_y_scaled)\n",
    "            get_family_MAE(test_plate, task_channel, \"Linear Regression\", \"Treated\", yhat_lr, df_test_treated_y_scaled)\n",
    "            \n",
    "            #get_family_MSE(yhat_lr,std_df_treated_y)\n",
    "            #get_family_MAE(yhat_lr,std_df_treated_y)\n",
    "            \n",
    "            print('profile_mock:')            \n",
    "            yhat_lr = pd.DataFrame(LR_model.predict(df_test_mock_x_scaled.values),columns=df_test_mock_y_scaled.columns)\n",
    "            print('Linear Reg MSE: {}'.format(mean_squared_error(yhat_lr,df_test_mock_y_scaled.values)))\n",
    "            print('Linear Reg MAE: {}'.format(mean_absolute_error(yhat_lr,df_test_mock_y_scaled.values)))\n",
    "            \n",
    "            print_results(test_plate, task_channel, 'Overall', 'Linear Regression', 'Mock', 'MSE', str(mean_squared_error(yhat_lr,df_test_mock_y_scaled.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'Linear Regression', 'Mock', 'MAE', str(mean_absolute_error(yhat_lr,df_test_mock_y_scaled.values)))\n",
    "            #get_family_MSE(yhat_lr,std_df_mock_y)\n",
    "            #get_family_MAE(yhat_lr,std_df_mock_y)\n",
    "            get_family_MSE(test_plate, task_channel, \"Linear Regression\", \"Mock\", yhat_lr,df_test_mock_y_scaled)\n",
    "            get_family_MAE(test_plate, task_channel, \"Linear Regression\", \"Mock\", yhat_lr,df_test_mock_y_scaled)\n",
    "                          \n",
    "            print('**************')\n",
    "            \n",
    "            print('**************')\n",
    "            print('Ridge')\n",
    "            print('profile_treated:') \n",
    "            yhat_ridge = pd.DataFrame(Ridge_model.predict(df_test_treated_x_scaled.values),columns=df_test_treated_y_scaled.columns)\n",
    "            print('Ridge MSE: {}'.format(mean_squared_error(yhat_ridge, df_test_treated_y_scaled.values)))\n",
    "            print('Ridge MAE: {}'.format(mean_absolute_error(yhat_ridge, df_test_treated_y_scaled.values)))\n",
    "                          \n",
    "            print_results(test_plate, task_channel, 'Overall', 'Ridge', 'Treated', 'MSE', str(mean_squared_error(yhat_ridge,df_test_treated_y_scaled.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'Ridge', 'Treated', 'MAE', str(mean_absolute_error(yhat_ridge,df_test_treated_y_scaled.values)))\n",
    "                    \n",
    "                          \n",
    "            get_family_MSE(test_plate, task_channel, \"Ridge\", \"Treated\", yhat_ridge, df_test_treated_y_scaled)\n",
    "            get_family_MAE(test_plate, task_channel, \"Ridge\", \"Treated\", yhat_ridge, df_test_treated_y_scaled)\n",
    "                          \n",
    "            #get_family_MSE(yhat_lr,std_df_treated_y)\n",
    "            #get_family_MAE(yhat_lr,std_df_treated_y)\n",
    "            \n",
    "            print('profile_mock:')            \n",
    "            yhat_ridge = pd.DataFrame(Ridge_model.predict(df_test_mock_x_scaled.values),columns=df_test_mock_y_scaled.columns)\n",
    "            print('Ridge MSE: {}'.format(mean_squared_error(yhat_ridge,df_test_mock_y_scaled.values)))\n",
    "            print('Ridge Reg MAE: {}'.format(mean_absolute_error(yhat_ridge,df_test_mock_y_scaled.values)))\n",
    "            #get_family_MSE(yhat_ridge,std_df_mock_y)\n",
    "            #get_family_MAE(yhat_ridge,std_df_mock_y)\n",
    "            print_results(test_plate, task_channel, 'Overall', 'Ridge', 'Mock', 'MSE', str(mean_squared_error(yhat_ridge,df_test_mock_y_scaled.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'Ridge', 'Mock', 'MAE', str(mean_absolute_error(yhat_ridge,df_test_mock_y_scaled.values)))\n",
    "                    \n",
    "                          \n",
    "            get_family_MSE(test_plate, task_channel, \"Ridge\", \"Mock\", yhat_ridge,df_test_mock_y_scaled)\n",
    "            get_family_MAE(test_plate, task_channel, \"Ridge\", \"Mock\", yhat_ridge,df_test_mock_y_scaled)\n",
    "            print('**************')\n",
    "            \n",
    "            print('**************')\n",
    "            print('DNN')\n",
    "            print('profile_treated:')\n",
    "            yhat_DNN = pd.DataFrame(DNN_model.predict(df_test_treated_x_scaled.values),columns=df_test_treated_y_scaled.columns)\n",
    "            print('DNN MSE: {}'.format(mean_squared_error(yhat_DNN,df_test_treated_y_scaled.values)))\n",
    "            print('DNN MAE: {}'.format(mean_absolute_error(yhat_DNN,df_test_treated_y_scaled.values)))\n",
    "            #get_family_MSE(yhat_DNN,std_df_treated_y)\n",
    "            #get_family_MAE(yhat_DNN,std_df_treated_y)\n",
    "            print_results(test_plate, task_channel, 'Overall', 'DNN', 'Treated', 'MSE', str(mean_squared_error(yhat_DNN,df_test_treated_y_scaled.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'DNN', 'Treated', 'MAE', str(mean_absolute_error(yhat_DNN,df_test_treated_y_scaled.values)))\n",
    "                    \n",
    "                          \n",
    "            get_family_MSE(test_plate, task_channel, \"DNN\", \"Treated\", yhat_DNN,df_test_treated_y_scaled)\n",
    "            get_family_MAE(test_plate, task_channel, \"DNN\", \"Treated\", yhat_DNN,df_test_treated_y_scaled)\n",
    "            \n",
    "            print('profile_mock:')\n",
    "            yhat_DNN = pd.DataFrame(DNN_model.predict(df_test_mock_x_scaled.values),columns=df_test_mock_y_scaled.columns)\n",
    "            print('DNN MSE: {}'.format(mean_squared_error(yhat_DNN,df_test_mock_y_scaled.values)))\n",
    "            print('DNN MAE: {}'.format(mean_absolute_error(yhat_DNN,df_test_mock_y_scaled.values)))\n",
    "                          \n",
    "                          \n",
    "            print_results(test_plate, task_channel, 'Overall', 'DNN', 'Mock', 'MSE', str(mean_squared_error(yhat_DNN,df_test_mock_y_scaled.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'DNN', 'Mock', 'MAE', str(mean_absolute_error(yhat_DNN,df_test_mock_y_scaled.values)))\n",
    "                    \n",
    "                          \n",
    "            get_family_MSE(test_plate, task_channel, \"DNN\", \"Mock\", yhat_DNN,df_test_mock_y_scaled)\n",
    "            get_family_MAE(test_plate, task_channel, \"DNN\", \"Mock\", yhat_DNN,df_test_mock_y_scaled)\n",
    "            #get_family_MSE(yhat_DNN,std_df_mock_y)\n",
    "            #get_family_MAE(yhat_DNN,std_df_mock_y)\n",
    "            print('**************')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.35s/it]\u001B[A\n",
      "100%|██████████| 3/3 [00:07<00:00,  2.44s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:00<00:01,  1.17it/s]\u001B[A\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.82s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:00<00:01,  1.16it/s]\u001B[A\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.19s/it]\u001B[A\n",
      " 20%|██        | 1/5 [02:45<11:00, 165.01s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.39s/it]\u001B[A\n",
      "100%|██████████| 3/3 [00:07<00:00,  2.55s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:00<00:01,  1.09it/s]\u001B[A\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.94s/it]\u001B[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:00<00:01,  1.06it/s]\u001B[A\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.30s/it]\u001B[A\n",
      " 20%|██        | 1/5 [04:40<18:40, 280.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26569.csv\n",
      "['mock', 'treated']\n",
      "['DMSO', 'BRD-K81665719-001-01-7', 'BRD-K27403344-001-01-3', 'BRD-K34495942-001-01-6', 'BRD-K89838866-001-01-3', 'BRD-K39341253-001-01-0', 'BRD-K86324038-001-01-0', 'BRD-K69236751-001-01-2', 'BRD-K08075306-001-01-8', 'BRD-K49933702-001-01-9', 'BRD-K33337505-001-01-0', 'BRD-K20152345-001-01-2', 'BRD-K33979639-001-01-8', 'BRD-K54901885-001-01-7']\n",
      "26569.csv\n",
      "AGP:\n",
      "**************\n",
      "LR\n",
      "profile_treated:\n",
      "Linear Reg MSE: 12825.745520416704\n",
      "Linear Reg MAE: 83.56430376717566\n",
      "profile_mock:\n",
      "Linear Reg MSE: 17559.049529901065\n",
      "Linear Reg MAE: 89.44889192406012\n",
      "**************\n",
      "**************\n",
      "Ridge\n",
      "profile_treated:\n",
      "Ridge MSE: 0.006536525018747467\n",
      "Ridge MAE: 0.05347437679236992\n",
      "profile_mock:\n",
      "Ridge MSE: 0.009918678434827832\n",
      "Ridge Reg MAE: 0.0661422701444501\n",
      "**************\n",
      "26572.csv\n",
      "['treated', 'mock']\n",
      "['BRD-K69073107-001-01-0', 'BRD-K71511528-001-01-6', 'BRD-K76407984-001-01-9', 'BRD-K26767410-001-01-1', 'BRD-K18092396-001-01-2', 'BRD-K29205927-001-01-5', 'BRD-K30158311-001-01-7', 'BRD-K11667007-001-01-9', 'BRD-K37032043-001-01-2', 'BRD-K30311018-001-01-8', 'BRD-K11182882-001-01-9', 'BRD-K36527604-001-01-3', 'BRD-K74987780-001-01-8', 'BRD-K14166322-001-01-7', 'BRD-K16573415-001-01-0', 'DMSO', 'BRD-K65975183-001-01-7', 'BRD-K55367197-001-01-3', 'BRD-K81429168-001-01-4', 'BRD-K24318151-001-01-8', 'BRD-K13778782-001-01-3', 'BRD-K37037521-001-01-2', 'BRD-K71709348-001-01-4', 'BRD-K31267215-001-01-2', 'BRD-K20101441-001-01-2', 'BRD-K43766497-001-01-8', 'BRD-K99766666-001-01-6', 'BRD-K57345620-001-01-4', 'BRD-K55748528-001-01-4', 'BRD-K29887111-001-01-6', 'BRD-K76158076-001-01-7', 'BRD-K36569842-001-01-7', 'BRD-K14867418-001-01-5', 'BRD-K09048421-001-01-4', 'BRD-K57844337-001-01-0', 'BRD-K39628918-001-01-7', 'BRD-K83289448-001-01-2', 'BRD-K90829984-001-01-6', 'BRD-K43664564-001-01-3', 'BRD-K82451136-001-01-6', 'BRD-K41072697-001-01-1', 'BRD-K40048343-001-01-0', 'BRD-K55627667-001-01-1', 'BRD-K29655604-001-01-1', 'BRD-K23140656-001-01-7', 'BRD-K05367966-001-01-3', 'BRD-K57567857-001-01-7', 'BRD-K38862476-001-01-7', 'BRD-K62909018-001-01-0', 'BRD-K94647778-001-01-8', 'BRD-K85612613-001-01-2', 'BRD-K78246835-001-01-0', 'BRD-K20361496-001-01-6', 'BRD-K93452754-001-01-2', 'BRD-K43364503-001-01-4', 'BRD-K59193920-001-01-3', 'BRD-K66211801-001-01-8', 'BRD-K69236751-001-01-2']\n",
      "26572.csv\n",
      "AGP:\n",
      "**************\n",
      "LR\n",
      "profile_treated:\n",
      "Linear Reg MSE: 564.0488177966425\n",
      "Linear Reg MAE: 14.585238557127642\n",
      "profile_mock:\n",
      "Linear Reg MSE: 17293.629500230334\n",
      "Linear Reg MAE: 95.53978543935366\n",
      "**************\n",
      "**************\n",
      "Ridge\n",
      "profile_treated:\n",
      "Ridge MSE: 0.006415132684993118\n",
      "Ridge MAE: 0.05525679539028368\n",
      "profile_mock:\n",
      "Ridge MSE: 0.009378110621985274\n",
      "Ridge Reg MAE: 0.06453764595241936\n",
      "**************\n",
      "26574.csv\n",
      "['treated', 'mock']\n",
      "['BRD-K29081836-001-01-0', 'BRD-K92404768-001-01-8', 'BRD-K79287319-001-01-8', 'BRD-K92163754-001-01-6', 'BRD-K65337672-001-01-8', 'BRD-K84760360-001-01-5', 'BRD-K98689898-001-01-1', 'BRD-K52525325-001-01-4', 'BRD-K55367197-001-01-3', 'BRD-K99766666-001-01-6', 'BRD-K57345620-001-01-4', 'BRD-K55748528-001-01-4', 'BRD-K29887111-001-01-6', 'BRD-K76158076-001-01-7', 'BRD-K36569842-001-01-7', 'BRD-K14867418-001-01-5', 'BRD-K09048421-001-01-4', 'BRD-K57844337-001-01-0', 'BRD-K65895220-001-01-2', 'BRD-K65859637-001-01-5', 'BRD-K31691428-001-01-6', 'BRD-K95760526-001-01-3', 'BRD-K90201499-001-01-6', 'BRD-K41151625-001-01-0', 'BRD-K30048461-001-01-8', 'BRD-K93783788-001-01-0', 'BRD-K20546229-001-01-6', 'BRD-K56903978-001-01-9', 'BRD-K68867948-001-01-9', 'BRD-K70039065-001-01-8', 'BRD-K03811780-001-01-6', 'BRD-K83484928-001-01-8', 'BRD-K02775862-001-01-5', 'DMSO', 'BRD-K39784035-001-01-6', 'BRD-K55627667-001-01-1', 'BRD-K29655604-001-01-1', 'BRD-K23140656-001-01-7', 'BRD-K05367966-001-01-3', 'BRD-K57567857-001-01-7', 'BRD-K38862476-001-01-7', 'BRD-K62909018-001-01-0', 'BRD-K79047281-001-01-0', 'BRD-K08739006-001-01-2', 'BRD-K16469239-001-01-5', 'BRD-K25222259-001-01-2', 'BRD-K09155468-001-01-1', 'BRD-K10072705-001-01-8', 'BRD-K28635638-001-01-4', 'BRD-K55011281-001-01-4', 'BRD-K73980669-001-01-0', 'BRD-K34257113-001-01-3', 'BRD-K89691421-001-01-4', 'BRD-K30278324-001-01-8', 'BRD-K31200284-001-01-9', 'BRD-K54662875-001-01-7', 'BRD-K45664515-001-01-1', 'BRD-K69655916-001-01-5', 'BRD-K74971043-001-01-5', 'BRD-K59223592-001-01-5', 'BRD-K44736866-001-01-3', 'BRD-K80190213-001-01-8', 'BRD-K53357510-001-01-6', 'BRD-K29764789-001-01-8', 'BRD-K81665719-001-01-7', 'BRD-K27403344-001-01-3', 'BRD-K34495942-001-01-6', 'BRD-K89838866-001-01-3', 'BRD-K39341253-001-01-0', 'BRD-K86324038-001-01-0', 'BRD-K14896981-001-01-1', 'BRD-K09541394-001-01-1', 'BRD-K65474083-001-01-2', 'BRD-K29548095-001-01-4', 'BRD-K89162199-001-01-6', 'BRD-K63636299-001-01-8', 'BRD-K06907095-001-01-9', 'BRD-K99027107-001-01-0', 'BRD-K58312590-001-01-9', 'BRD-K32797868-001-01-7', 'BRD-K74568880-001-01-5', 'BRD-K90488649-001-01-8', 'BRD-K70088258-001-01-2', 'BRD-K97780373-001-01-3', 'BRD-K04830910-001-01-8', 'BRD-K90748516-001-01-5']\n",
      "26574.csv\n",
      "AGP:\n",
      "**************\n",
      "LR\n",
      "profile_treated:\n",
      "Linear Reg MSE: 18626.637226759114\n",
      "Linear Reg MAE: 92.47371578128207\n",
      "profile_mock:\n",
      "Linear Reg MSE: 16250.044655132446\n",
      "Linear Reg MAE: 96.9782199946028\n",
      "**************\n",
      "**************\n",
      "Ridge\n",
      "profile_treated:\n",
      "Ridge MSE: 0.007389494858061301\n",
      "Ridge MAE: 0.059823375620111786\n",
      "profile_mock:\n",
      "Ridge MSE: 0.007397716781321351\n",
      "Ridge Reg MAE: 0.0575265408155228\n",
      "**************\n",
      "26569.csv\n",
      "['mock', 'treated']\n",
      "['DMSO', 'BRD-K81665719-001-01-7', 'BRD-K27403344-001-01-3', 'BRD-K34495942-001-01-6', 'BRD-K89838866-001-01-3', 'BRD-K39341253-001-01-0', 'BRD-K86324038-001-01-0', 'BRD-K69236751-001-01-2', 'BRD-K08075306-001-01-8', 'BRD-K49933702-001-01-9', 'BRD-K33337505-001-01-0', 'BRD-K20152345-001-01-2', 'BRD-K33979639-001-01-8', 'BRD-K54901885-001-01-7']\n",
      "26569.csv\n",
      "DNA:\n",
      "**************\n",
      "LR\n",
      "profile_treated:\n",
      "Linear Reg MSE: 1831.2847103564084\n",
      "Linear Reg MAE: 32.06414222025862\n",
      "empty family Granularity\n",
      "empty family Granularity\n",
      "profile_mock:\n",
      "Linear Reg MSE: 9340.901408026666\n",
      "Linear Reg MAE: 67.29880656720077\n",
      "empty family Granularity\n",
      "empty family Granularity\n",
      "**************\n",
      "**************\n",
      "Ridge\n",
      "profile_treated:\n",
      "Ridge MSE: 0.008294456199871646\n",
      "Ridge MAE: 0.06406159324384822\n",
      "empty family Granularity\n",
      "empty family Granularity\n",
      "profile_mock:\n",
      "Ridge MSE: 0.016586705343275554\n",
      "Ridge Reg MAE: 0.08537035875989506\n",
      "empty family Granularity\n",
      "empty family Granularity\n",
      "**************\n",
      "26572.csv\n",
      "['treated', 'mock']\n",
      "['BRD-K69073107-001-01-0', 'BRD-K71511528-001-01-6', 'BRD-K76407984-001-01-9', 'BRD-K26767410-001-01-1', 'BRD-K18092396-001-01-2', 'BRD-K29205927-001-01-5', 'BRD-K30158311-001-01-7', 'BRD-K11667007-001-01-9', 'BRD-K37032043-001-01-2', 'BRD-K30311018-001-01-8', 'BRD-K11182882-001-01-9', 'BRD-K36527604-001-01-3', 'BRD-K74987780-001-01-8', 'BRD-K14166322-001-01-7', 'BRD-K16573415-001-01-0', 'DMSO', 'BRD-K65975183-001-01-7', 'BRD-K55367197-001-01-3', 'BRD-K81429168-001-01-4', 'BRD-K24318151-001-01-8', 'BRD-K13778782-001-01-3', 'BRD-K37037521-001-01-2', 'BRD-K71709348-001-01-4', 'BRD-K31267215-001-01-2', 'BRD-K20101441-001-01-2', 'BRD-K43766497-001-01-8', 'BRD-K99766666-001-01-6', 'BRD-K57345620-001-01-4', 'BRD-K55748528-001-01-4', 'BRD-K29887111-001-01-6', 'BRD-K76158076-001-01-7', 'BRD-K36569842-001-01-7', 'BRD-K14867418-001-01-5', 'BRD-K09048421-001-01-4', 'BRD-K57844337-001-01-0', 'BRD-K39628918-001-01-7', 'BRD-K83289448-001-01-2', 'BRD-K90829984-001-01-6', 'BRD-K43664564-001-01-3', 'BRD-K82451136-001-01-6', 'BRD-K41072697-001-01-1', 'BRD-K40048343-001-01-0', 'BRD-K55627667-001-01-1', 'BRD-K29655604-001-01-1', 'BRD-K23140656-001-01-7', 'BRD-K05367966-001-01-3', 'BRD-K57567857-001-01-7', 'BRD-K38862476-001-01-7', 'BRD-K62909018-001-01-0', 'BRD-K94647778-001-01-8', 'BRD-K85612613-001-01-2', 'BRD-K78246835-001-01-0', 'BRD-K20361496-001-01-6', 'BRD-K93452754-001-01-2', 'BRD-K43364503-001-01-4', 'BRD-K59193920-001-01-3', 'BRD-K66211801-001-01-8', 'BRD-K69236751-001-01-2']\n",
      "26572.csv\n",
      "DNA:\n",
      "**************\n",
      "LR\n",
      "profile_treated:\n",
      "Linear Reg MSE: 196.03488811289705\n",
      "Linear Reg MAE: 9.730911738346347\n",
      "empty family Granularity\n",
      "empty family Granularity\n",
      "profile_mock:\n",
      "Linear Reg MSE: 1794.575314079134\n",
      "Linear Reg MAE: 30.39219228672681\n",
      "empty family Granularity\n",
      "empty family Granularity\n",
      "**************\n",
      "**************\n",
      "Ridge\n",
      "profile_treated:\n",
      "Ridge MSE: 0.005439246308883385\n",
      "Ridge MAE: 0.054056389405390176\n",
      "empty family Granularity\n",
      "empty family Granularity\n",
      "profile_mock:\n",
      "Ridge MSE: 0.016912572247941644\n",
      "Ridge Reg MAE: 0.09721515839923073\n",
      "empty family Granularity\n",
      "empty family Granularity\n",
      "**************\n",
      "26574.csv\n",
      "['treated', 'mock']\n",
      "['BRD-K29081836-001-01-0', 'BRD-K92404768-001-01-8', 'BRD-K79287319-001-01-8', 'BRD-K92163754-001-01-6', 'BRD-K65337672-001-01-8', 'BRD-K84760360-001-01-5', 'BRD-K98689898-001-01-1', 'BRD-K52525325-001-01-4', 'BRD-K55367197-001-01-3', 'BRD-K99766666-001-01-6', 'BRD-K57345620-001-01-4', 'BRD-K55748528-001-01-4', 'BRD-K29887111-001-01-6', 'BRD-K76158076-001-01-7', 'BRD-K36569842-001-01-7', 'BRD-K14867418-001-01-5', 'BRD-K09048421-001-01-4', 'BRD-K57844337-001-01-0', 'BRD-K65895220-001-01-2', 'BRD-K65859637-001-01-5', 'BRD-K31691428-001-01-6', 'BRD-K95760526-001-01-3', 'BRD-K90201499-001-01-6', 'BRD-K41151625-001-01-0', 'BRD-K30048461-001-01-8', 'BRD-K93783788-001-01-0', 'BRD-K20546229-001-01-6', 'BRD-K56903978-001-01-9', 'BRD-K68867948-001-01-9', 'BRD-K70039065-001-01-8', 'BRD-K03811780-001-01-6', 'BRD-K83484928-001-01-8', 'BRD-K02775862-001-01-5', 'DMSO', 'BRD-K39784035-001-01-6', 'BRD-K55627667-001-01-1', 'BRD-K29655604-001-01-1', 'BRD-K23140656-001-01-7', 'BRD-K05367966-001-01-3', 'BRD-K57567857-001-01-7', 'BRD-K38862476-001-01-7', 'BRD-K62909018-001-01-0', 'BRD-K79047281-001-01-0', 'BRD-K08739006-001-01-2', 'BRD-K16469239-001-01-5', 'BRD-K25222259-001-01-2', 'BRD-K09155468-001-01-1', 'BRD-K10072705-001-01-8', 'BRD-K28635638-001-01-4', 'BRD-K55011281-001-01-4', 'BRD-K73980669-001-01-0', 'BRD-K34257113-001-01-3', 'BRD-K89691421-001-01-4', 'BRD-K30278324-001-01-8', 'BRD-K31200284-001-01-9', 'BRD-K54662875-001-01-7', 'BRD-K45664515-001-01-1', 'BRD-K69655916-001-01-5', 'BRD-K74971043-001-01-5', 'BRD-K59223592-001-01-5', 'BRD-K44736866-001-01-3', 'BRD-K80190213-001-01-8', 'BRD-K53357510-001-01-6', 'BRD-K29764789-001-01-8', 'BRD-K81665719-001-01-7', 'BRD-K27403344-001-01-3', 'BRD-K34495942-001-01-6', 'BRD-K89838866-001-01-3', 'BRD-K39341253-001-01-0', 'BRD-K86324038-001-01-0', 'BRD-K14896981-001-01-1', 'BRD-K09541394-001-01-1', 'BRD-K65474083-001-01-2', 'BRD-K29548095-001-01-4', 'BRD-K89162199-001-01-6', 'BRD-K63636299-001-01-8', 'BRD-K06907095-001-01-9', 'BRD-K99027107-001-01-0', 'BRD-K58312590-001-01-9', 'BRD-K32797868-001-01-7', 'BRD-K74568880-001-01-5', 'BRD-K90488649-001-01-8', 'BRD-K70088258-001-01-2', 'BRD-K97780373-001-01-3', 'BRD-K04830910-001-01-8', 'BRD-K90748516-001-01-5']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-14-917a1dbf0638>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'csvs/'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mS_MinMax\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-13-b3713d168737>\u001B[0m in \u001B[0;36mmain\u001B[1;34m(path, scale_method)\u001B[0m\n\u001B[0;32m     61\u001B[0m             \u001B[0mdf_train_y\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Data'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34mf'{plate_number}_{task_channel}_train_y.csv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m             \u001B[0mchannel_task_x_mock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Data'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34mf'{plate_number}_{task_channel}_test_x_mock.csv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 63\u001B[1;33m             \u001B[0mchannel_task_x_treated\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Data'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34mf'{plate_number}_{task_channel}_test_x_treated.csv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     64\u001B[0m             \u001B[0mchannel_task_y_mock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Data'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34mf'{plate_number}_{task_channel}_test_y_mock.csv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m             \u001B[0mchannel_task_y_treated\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Data'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34mf'{plate_number}_{task_channel}_test_y_treated.csv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\niko\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36mto_csv\u001B[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001B[0m\n\u001B[0;32m   3202\u001B[0m             \u001B[0mdecimal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdecimal\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3203\u001B[0m         )\n\u001B[1;32m-> 3204\u001B[1;33m         \u001B[0mformatter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3205\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3206\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mpath_or_buf\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\niko\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    202\u001B[0m             )\n\u001B[0;32m    203\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 204\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_save\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    205\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    206\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\niko\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001B[0m in \u001B[0;36m_save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    323\u001B[0m                 \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    324\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 325\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_save_chunk\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstart_i\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend_i\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    326\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    327\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_save_chunk\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_i\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend_i\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\niko\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001B[0m in \u001B[0;36m_save_chunk\u001B[1;34m(self, start_i, end_i)\u001B[0m\n\u001B[0;32m    354\u001B[0m         )\n\u001B[0;32m    355\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 356\u001B[1;33m         \u001B[0mlibwriters\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite_csv_rows\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mix\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnlevels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcols\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwriter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mpandas\\_libs\\writers.pyx\u001B[0m in \u001B[0;36mpandas._libs.writers.write_csv_rows\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs('Models', exist_ok=True)\n",
    "os.makedirs('Results', exist_ok=True)\n",
    "main('csvs/', S_STD)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}