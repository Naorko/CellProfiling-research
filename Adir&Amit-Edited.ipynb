{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are importing all relevant packages for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T09:42:39.510247Z",
     "start_time": "2020-08-26T09:42:30.284358Z"
    }
   },
   "outputs": [],
   "source": [
    "# connections and OS\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import os\n",
    "import sqlite3\n",
    "#import csv\n",
    "\n",
    "#utils (Pandas,numpy,tqdm)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "#visualize \n",
    "#import seaborn as sns\n",
    "\n",
    "#preprocessing, metrices and splits \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "#ML models:\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "\n",
    "#tensorflow layer, callbacks and layers\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change directory to project directory within the department cluster (SLURM) and define to constant variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T09:42:49.843122Z",
     "start_time": "2020-08-26T09:42:49.836703Z"
    }
   },
   "outputs": [],
   "source": [
    "#change directory to project directory within the department cluster (SLURM)\n",
    "PROJECT_DIRECTORY = r'C:\\Users\\Niko\\Desktop\\plates'\n",
    "CHANNELS = [\"AGP\",\"DNA\",\"ER\",\"Mito\",\"RNA\"]\n",
    "LABEL_FIELD = 'Metadata_ASSAY_WELL_ROLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PROJECT_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T09:42:50.672093Z",
     "start_time": "2020-08-26T09:42:50.661259Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_data(df):\n",
    "    \"\"\"\n",
    "    This function is scaling the data using two methods: STD and MinMax\n",
    "    df: dataFrame     \n",
    "    return: two scaled dataframes, the first one according to StandardScaler and the second according to MinMaxScaler\n",
    "    \"\"\"\n",
    "    std_scalar = StandardScaler()\n",
    "    minMax_scalar = MinMaxScaler()\n",
    "    \n",
    "    std_df = pd.DataFrame(std_scalar.fit_transform(df),columns=df.columns)\n",
    "    minmax_df = pd.DataFrame(minMax_scalar.fit_transform(df),columns=df.columns)    \n",
    "    std_df.fillna(0,inplace=True)\n",
    "    minmax_df.fillna(0,inplace=True)\n",
    "    \n",
    "    return std_df,minmax_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:02:32.110621Z",
     "start_time": "2020-08-26T10:02:32.088820Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_channels_x_and_y(filename, task_channel):\n",
    "    \"\"\"\n",
    "    This function is responsible for splitting five channels into four channels as train and the remaining channel to test\n",
    "    filename: file path to the cell table from a single plate\n",
    "    task_channel: the current channel that we aim to predict\n",
    "    \n",
    "    Notably: In order to avoid leakage we drop all 'correlation features\n",
    "    return: seperated dataframes x_features and y_df. \n",
    "            x_features: contains all available features excluding the features related to 'task_channel' we aim to predict\n",
    "            y_df: contains all available features related to 'task_channel' only\n",
    "    \"\"\"\n",
    "    dict_channel = {}\n",
    "    \n",
    "    #df = pd.read_csv(filename+\".csv\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.set_index(['ImageNumber', 'ObjectNumber'])\n",
    "    df.drop(['TableNumber'],inplace=True,axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Data Preperation\n",
    "    general_featuers = df.iloc[:, 0:52]\n",
    "    general_featuers[LABEL_FIELD] = df[LABEL_FIELD]\n",
    "    df = df.iloc[:, 52:]\n",
    "    for channel in CHANNELS:\n",
    "        dict_channel[channel] = df[[col for col in df.columns if channel in col]]\n",
    "\n",
    "    ready_channel_features = []\n",
    "    for feature_name in dict_channel:\n",
    "        if feature_name != task_channel:\n",
    "            curr_channel_features = dict_channel[feature_name]\n",
    "            curr_channel_features = curr_channel_features[[col for col in curr_channel_features.columns if task_channel not in col]]\n",
    "            ready_channel_features.append(curr_channel_features)\n",
    "    x_features_df = ready_channel_features[0]    \n",
    "    for i in range(1, len(ready_channel_features)):\n",
    "        x_features_df = x_features_df.join(ready_channel_features[i], how='outer', lsuffix='_left',\n",
    "                                           rsuffix='_right')\n",
    "    x_features_df = general_featuers.join(x_features_df, how='outer', lsuffix='_left', rsuffix='_right')\n",
    "    y_df = dict_channel[task_channel]\n",
    "    corr_cols = [c for c in y_df.columns if 'correlation' not in c.lower()]\n",
    "    y_df = y_df[corr_cols]\n",
    "    return x_features_df, y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following three cells we are creating three ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:13:53.617800Z",
     "start_time": "2020-08-26T10:13:53.610909Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_LR(df_train_X, df_train_Y):  \n",
    "    \"\"\"\n",
    "    In this cell we are creating and training a linear regression model        \n",
    "    df_train_X: contains all available features excluding the features related to 'task_channel' we aim to predict (train)\n",
    "    df_train_Y: contains all available features related to 'task_channel' only for the train\n",
    "    \n",
    "    \n",
    "    return: trained linear regression model\n",
    "    \"\"\"\n",
    "    LR_model = LinearRegression()    \n",
    "    LR_model.fit(df_train_X.values,df_train_Y.values)\n",
    "    return LR_model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:13:53.890850Z",
     "start_time": "2020-08-26T10:13:53.884664Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_Ridge(df_train_X, df_train_Y):\n",
    "    \"\"\"    \n",
    "    In this cell we are creating and training a ridge regression model    \n",
    "    \n",
    "    \n",
    "    df_train_X: contains all available features excluding the features related to 'task_channel' we aim to predict (train)\n",
    "    df_train_Y: contains all available features related to 'task_channel' only for the train    \n",
    "    \n",
    "    return: trained ridge regression model\n",
    "    \"\"\"\n",
    "    Ridge_model = Ridge()    \n",
    "    Ridge_model.fit(X=df_train_X.values,y=df_train_Y.values)    \n",
    "    return Ridge_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:13:54.196170Z",
     "start_time": "2020-08-26T10:13:54.176523Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_model_dnn(task_channel,df_train_X, df_train_Y,test_plate):\n",
    "    \"\"\"    \n",
    "    In this cell we are creating and training a multi layer perceptron (we refer to it as deep neural network, DNN) model\n",
    "    \n",
    "    task_channel: the current channel that we aim to predict\n",
    "    df_train_X: contains all available features excluding the features related to 'task_channel' we aim to predict (train)\n",
    "    df_train_Y: contains all available features related to 'task_channel' only for the train\n",
    "    test_plate: the ID of a given plate. This information assist us while printing the results.\n",
    "    \n",
    "    return: trained dnn model\n",
    "    \"\"\"\n",
    "    # Stracture of the network#\n",
    "    inputs = Input(shape=(df_train_X.shape[1],))\n",
    "    dense1 = Dense(512,activation = 'relu')(inputs)\n",
    "    dense2 = Dense(256,activation = 'relu')(dense1)\n",
    "    dense3 = Dense(128,activation = 'relu')(dense2)    \n",
    "    dense4 = Dense(100,activation = 'relu')(dense3)\n",
    "    dense5 = Dense(50,activation = 'relu')(dense4)\n",
    "    dense6 = Dense(25,activation = 'relu')(dense5)\n",
    "    dense7 = Dense(10,activation = 'relu')(dense6)\n",
    "    predictions = Dense(df_train_Y.shape[1],activation='sigmoid')(dense7)\n",
    "    \n",
    "    #model compiliation\n",
    "    model = Model(inputs=inputs,outputs = predictions)\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    \n",
    "    #model training    \n",
    "    test_plate_number = test_plate[:5]\n",
    "    folder = os.path.join(PROJECT_DIRECTORY, 'Models')\n",
    "    filepath = os.path.join(folder, f'{test_plate_number}_{task_channel}.h5')\n",
    "    my_callbacks = [ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)]\n",
    "    model.fit(df_train_X,df_train_Y,epochs = 5,batch_size=1024*8,verbose=0,shuffle=True,validation_split=0.2,callbacks=my_callbacks)\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:13:54.704037Z",
     "start_time": "2020-08-26T10:13:54.691051Z"
    }
   },
   "outputs": [],
   "source": [
    "#    print_results(test_plate_number, task_channel, \"Overall\", \"DNN\", \"None\", \"MSE\", str(mean_squared_error(model_pred,channel_task_y)))\n",
    "def print_results(plate_number, channel, family, model, _type, metric, value):\n",
    "    \"\"\"\n",
    "    This function is creating a csv named: 'results' that contains all of the models’ performance (e.g. MSE) for each plate and each family of attributes\n",
    "    plate_number: ID of palte\n",
    "    channel: The channel we aim to predict\n",
    "    family: features united by their charactheristics (e.g., Granularity, Texture)\n",
    "    model: the model name\n",
    "    _type: scaling method (e.g., MinMax Scaler or StandardScaler)\n",
    "    metric: MSE/MAE\n",
    "    value: value of the metric error    \n",
    "    \"\"\"\n",
    "    results_path = os.path.join(PROJECT_DIRECTORY, 'Results')\n",
    "    file_path = os.path.join(results_path, 'results.csv')\n",
    "    files_list = os.listdir(results_path)\n",
    "    if 'results.csv' not in files_list:\n",
    "        file1 = open(file_path,\"a+\")     \n",
    "        file1.write(\"Plate,Channel,Family,Model,Type,Metric,Value \\n\")\n",
    "        file1.write(plate_number+\",\"+channel+\",\"+family+\",\"+model+\",\"+_type+\",\"+metric+\",\"+value+\"\\n\")\n",
    "        file1.close()\n",
    "    else:\n",
    "        file1 = open(file_path, \"a+\")\n",
    "        file1.write(plate_number+\",\"+channel+\",\"+family+\",\"+model+\",\"+_type+\",\"+metric+\",\"+value+\"\\n\")\n",
    "        file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:13:56.047905Z",
     "start_time": "2020-08-26T10:13:56.029632Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_family_MSE(test_plate_number, task_channel, model, _type, df, channel_task_y):    \n",
    "    \"\"\"\n",
    "    This function is calculating the MSE/MAE measures for plates based on different models\n",
    "    test_plate_number: ID of the examine plate\n",
    "    task_channel: Channel we aim to predict\n",
    "    model: model name\n",
    "    _type: scaling method (e.g., MinMax Scaler or StandardScaler)\n",
    "    df: prediction of any given ML model which aim to predict the channel_task_y\n",
    "    channel_task_y: features corresponding to the 'task channel' (channel we aim to predict)    \n",
    "    \"\"\"\n",
    "    Families = {'Granularity':[],\n",
    "               'Intensity':[],\n",
    "               'Location':[],\n",
    "               'RadialDistribution':[],\n",
    "               'Texture':[]}\n",
    "\n",
    "    for name in (channel_task_y.columns):\n",
    "        if '_Granularity' in name:\n",
    "            Families['Granularity'].append(name)\n",
    "        elif '_Intensity' in name:\n",
    "            Families['Intensity'].append(name)\n",
    "        elif '_Location' in name:\n",
    "            Families['Location'].append(name)        \n",
    "        elif '_RadialDistribution' in name:\n",
    "            Families['RadialDistribution'].append(name)\n",
    "        elif '_Texture' in name:\n",
    "            Families['Texture'].append(name)\n",
    "            \n",
    "    for key in Families.keys():\n",
    "        try:            \n",
    "            print_results(test_plate_number, task_channel, key, model, _type, \"MSE\", str(mean_squared_error(df[Families[key]],channel_task_y[Families[key]])))\n",
    "        except:\n",
    "            if len(Families[key]) == 0:\n",
    "                print('empty family {}'.format(key))\n",
    "            else:\n",
    "                print('problem in mse key')\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:13:56.616395Z",
     "start_time": "2020-08-26T10:13:56.597732Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_family_MAE(test_plate_number, task_channel, model, _type, df, channel_task_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is calculating the MSE/MAE measures for plates based on different models\n",
    "    test_plate_number: ID of the examine plate\n",
    "    task_channel: Channel we aim to predict\n",
    "    model: model name\n",
    "    _type: scaling method (e.g., MinMax Scaler or StandardScaler)\n",
    "    df: prediction of any given ML model which aim to predict the channel_task_y\n",
    "    channel_task_y: features corresponding to the 'task channel' (channel we aim to predict)    \n",
    "    \"\"\"\n",
    "    \n",
    "    Families = {'Granularity':[],\n",
    "               'Intensity':[],\n",
    "               'Location':[],\n",
    "               'RadialDistribution':[],\n",
    "               'Texture':[]}\n",
    "\n",
    "    for name in (channel_task_y.columns):\n",
    "        if '_Granularity' in name:\n",
    "            Families['Granularity'].append(name)\n",
    "        elif '_Intensity' in name:\n",
    "            Families['Intensity'].append(name)\n",
    "        elif '_Location' in name:\n",
    "            Families['Location'].append(name)        \n",
    "        elif '_RadialDistribution' in name:\n",
    "            Families['RadialDistribution'].append(name)\n",
    "        elif '_Texture' in name:\n",
    "            Families['Texture'].append(name)\n",
    "            \n",
    "    for key in Families.keys():\n",
    "        try:            \n",
    "            print_results(test_plate_number, task_channel, key, model, _type, \"MAE\", str(mean_absolute_error(df[Families[key]],channel_task_y[Families[key]])))\n",
    "        except:\n",
    "            if len(Families[key]) == 0:\n",
    "                print('empty family {}'.format(key))\n",
    "            else:\n",
    "                print('problem in mae key')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:13:57.225714Z",
     "start_time": "2020-08-26T10:13:57.160130Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(path,scale_method):\n",
    "    \"\"\"\n",
    "    This is the main function of the preprocessing steps.\n",
    "    This function will iterate all over the sqlite files and do the following:\n",
    "    1) prepate train + test files\n",
    "    2) scale train + test files (x + y values separately)\n",
    "    3) return: \n",
    "        task_channel -> string, reflect the relevant channel for test. For example, 'AGP'\n",
    "        df_train_X -> DataFrame, (instances,features) for the train set\n",
    "        df_train_Y -> DataFrame, (instances,labels) for the train set\n",
    "        channel_task_x -> DataFrame, (instances,features) for the test set\n",
    "        channel_task_y -> DataFrame, (instances,labels) for the test set\n",
    "    \"\"\"\n",
    "\n",
    "    csv_files= [_ for _ in os.listdir(path) if _.endswith(\".csv\")]\n",
    "    for task_channel in tqdm(CHANNELS):        \n",
    "        # This is the current file that we will predict        \n",
    "        for test_plate in csv_files:\n",
    "            print(test_plate)\n",
    "\n",
    "            channel_task_x, channel_task_y = split_channels_x_and_y(path + test_plate, task_channel)\n",
    "            print(channel_task_x[LABEL_FIELD].unique())\n",
    "\n",
    "            channel_task_x_mock = channel_task_x[channel_task_x[LABEL_FIELD]=='mock']\n",
    "            channel_task_x_treated = channel_task_x[channel_task_x[LABEL_FIELD]=='treated']\n",
    "\n",
    "            channel_task_y_mock = channel_task_y.loc[channel_task_x_mock.index]\n",
    "            channel_task_y_treated = channel_task_y.loc[channel_task_x_treated.index]\n",
    "\n",
    "            channel_task_x_mock.drop([LABEL_FIELD],inplace=True,axis=1)\n",
    "            channel_task_x_treated.drop([LABEL_FIELD],inplace=True,axis=1)\n",
    "\n",
    "\n",
    "            std_df_treated_x ,min_max_df_treated_x = scale_data(channel_task_x_treated)\n",
    "            std_df_treated_y ,min_max_df_treated_y = scale_data(channel_task_y_treated)\n",
    "            std_df_mock_x ,min_max_df_mock_x = scale_data(channel_task_x_mock)\n",
    "            std_df_mock_y ,min_max_df_mock_y = scale_data(channel_task_y_mock)\n",
    "\n",
    "                \n",
    "        # This is all other files X input\n",
    "            list_x_df = []\n",
    "            list_y_df = []\n",
    "            \n",
    "            \n",
    "            for train_plate in tqdm(csv_files):\n",
    "                if train_plate!=test_plate:\n",
    "                    curr_x, curr_y = split_channels_x_and_y(path + train_plate, task_channel)\n",
    "                    curr_x = curr_x[curr_x[LABEL_FIELD]=='mock']\n",
    "                    curr_y = curr_y.loc[curr_x.index]\n",
    "                    curr_x.drop([LABEL_FIELD],inplace=True,axis=1)\n",
    "\n",
    "                    list_x_df.append(curr_x)\n",
    "                    list_y_df.append(curr_y)\n",
    "            \n",
    "            df_train_X = pd.concat(list_x_df)\n",
    "            df_train_Y = pd.concat(list_y_df)   \n",
    "            \n",
    "             # Scale for training set#\n",
    "            std_df ,min_max_df = scale_data(df_train_X)            \n",
    "            std_df_y ,min_max_df_y = scale_data(df_train_Y)\n",
    "            \n",
    "            #Scale for testing set - treated#\n",
    "            std_df_channel_task_treated ,min_max_df_channel_task_treated = scale_data(channel_task_x_treated)\n",
    "            std_df_y_test_treated ,min_max_df_y_test_treated = scale_data(channel_task_y_treated)\n",
    "            \n",
    "            #Scale for testing set - mock#\n",
    "            std_df_channel_task_mock ,min_max_df_channel_task_mock = scale_data(channel_task_x_mock)\n",
    "            std_df_y_test_mock ,min_max_df_y_test_mock = scale_data(channel_task_y_mock)   \n",
    "            \n",
    "            if scale_method == 'MinMax':\n",
    "                #train set#\n",
    "                df_train_X = min_max_df\n",
    "                df_train_Y = min_max_df_y\n",
    "                \n",
    "                #treated #\n",
    "                df_test_X_treated = min_max_df_channel_task_treated\n",
    "                df_test_Y_treated = min_max_df_y_test_treated\n",
    "                \n",
    "                #mock#                \n",
    "                df_test_X_mock = min_max_df_channel_task_mock\n",
    "                df_test_Y_mock = min_max_df_y_test_mock\n",
    "                \n",
    "                \n",
    "            elif scale_method == 'Std':\n",
    "                #train set#\n",
    "                df_train_X = std_df\n",
    "                df_train_Y = std_df_y\n",
    "                \n",
    "                #treated #\n",
    "                df_test_X_treated = std_df_channel_task_treated\n",
    "                df_test_Y_treated = std_df_y_test_treated\n",
    "                \n",
    "                #mock#                \n",
    "                df_test_X_mock = std_df_channel_task_mock\n",
    "                df_test_Y_mock = std_df_y_test_mock\n",
    "                \n",
    "                \n",
    "        # Model Creation - AVG MSE for each model:\n",
    "            print(test_plate)\n",
    "            print(task_channel+\":\")\n",
    "            LR_model = create_LR(df_train_X, df_train_Y)\n",
    "            Ridge_model = create_Ridge(df_train_X, df_train_Y)\n",
    "            DNN_model = create_model_dnn(task_channel,df_train_X, df_train_Y,test_plate)\n",
    "#             svr_model = create_SVR(task_channel,df_train_X, df_train_Y, channel_task_x, channel_task_y)\n",
    "            #return task_channel,df_train_X, df_train_Y, channel_task_x, channel_task_y\n",
    "    \n",
    "            print('**************')\n",
    "            print('LR')\n",
    "            print('profile_treated:') \n",
    "            yhat_lr = pd.DataFrame(LR_model.predict(std_df_treated_x.values),columns=std_df_treated_y.columns)                           \n",
    "            print('Linear Reg MSE: {}'.format(mean_squared_error(yhat_lr,std_df_treated_y.values)))  \n",
    "            print('Linear Reg MAE: {}'.format(mean_absolute_error(yhat_lr,std_df_treated_y.values)))\n",
    "            \n",
    "            print_results(test_plate, task_channel, 'Overall', 'Linear Regression', 'Treated', 'MSE', str(mean_squared_error(yhat_lr,std_df_treated_y.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'Linear Regression', 'Treated', 'MAE', str(mean_absolute_error(yhat_lr,std_df_treated_y.values)))\n",
    "            \n",
    "            get_family_MSE(test_plate, task_channel, \"Linear Regression\", \"Treated\", yhat_lr,std_df_treated_y)\n",
    "            get_family_MAE(test_plate, task_channel, \"Linear Regression\", \"Treated\", yhat_lr,std_df_treated_y)\n",
    "            \n",
    "            #get_family_MSE(yhat_lr,std_df_treated_y)\n",
    "            #get_family_MAE(yhat_lr,std_df_treated_y)\n",
    "            \n",
    "            print('profile_mock:')            \n",
    "            yhat_lr = pd.DataFrame(LR_model.predict(std_df_mock_x.values),columns=std_df_mock_y.columns)   \n",
    "            print('Linear Reg MSE: {}'.format(mean_squared_error(yhat_lr,std_df_mock_y.values)))  \n",
    "            print('Linear Reg MAE: {}'.format(mean_absolute_error(yhat_lr,std_df_mock_y.values)))  \n",
    "            \n",
    "            print_results(test_plate, task_channel, 'Overall', 'Linear Regression', 'Mock', 'MSE', str(mean_squared_error(yhat_lr,std_df_mock_y.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'Linear Regression', 'Mock', 'MAE', str(mean_absolute_error(yhat_lr,std_df_mock_y.values)))            \n",
    "            #get_family_MSE(yhat_lr,std_df_mock_y)\n",
    "            #get_family_MAE(yhat_lr,std_df_mock_y)\n",
    "            get_family_MSE(test_plate, task_channel, \"Linear Regression\", \"Mock\", yhat_lr,std_df_mock_y)\n",
    "            get_family_MAE(test_plate, task_channel, \"Linear Regression\", \"Mock\", yhat_lr,std_df_mock_y)\n",
    "                          \n",
    "            print('**************')\n",
    "            \n",
    "            print('**************')\n",
    "            print('Ridge')\n",
    "            print('profile_treated:') \n",
    "            yhat_ridge = pd.DataFrame(Ridge_model.predict(std_df_treated_x.values),columns=std_df_treated_y.columns)                           \n",
    "            print('Ridge MSE: {}'.format(mean_squared_error(yhat_ridge,std_df_treated_y.values)))  \n",
    "            print('Ridge MAE: {}'.format(mean_absolute_error(yhat_ridge,std_df_treated_y.values)))  \n",
    "                          \n",
    "            print_results(test_plate, task_channel, 'Overall', 'Ridge', 'Treated', 'MSE', str(mean_squared_error(yhat_ridge,std_df_treated_y.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'Ridge', 'Treated', 'MAE', str(mean_absolute_error(yhat_ridge,std_df_treated_y.values)))\n",
    "                    \n",
    "                          \n",
    "            get_family_MSE(test_plate, task_channel, \"Ridge\", \"Treated\", yhat_ridge,std_df_treated_y)\n",
    "            get_family_MAE(test_plate, task_channel, \"Ridge\", \"Treated\", yhat_ridge,std_df_treated_y)\n",
    "                          \n",
    "            #get_family_MSE(yhat_lr,std_df_treated_y)\n",
    "            #get_family_MAE(yhat_lr,std_df_treated_y)\n",
    "            \n",
    "            print('profile_mock:')            \n",
    "            yhat_ridge = pd.DataFrame(Ridge_model.predict(std_df_mock_x.values),columns=std_df_mock_y.columns)   \n",
    "            print('Ridge MSE: {}'.format(mean_squared_error(yhat_ridge,std_df_mock_y.values)))  \n",
    "            print('Ridge Reg MAE: {}'.format(mean_absolute_error(yhat_ridge,std_df_mock_y.values)))  \n",
    "            #get_family_MSE(yhat_ridge,std_df_mock_y)\n",
    "            #get_family_MAE(yhat_ridge,std_df_mock_y)\n",
    "            print_results(test_plate, task_channel, 'Overall', 'Ridge', 'Mock', 'MSE', str(mean_squared_error(yhat_ridge,std_df_mock_y.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'Ridge', 'Mock', 'MAE', str(mean_absolute_error(yhat_ridge,std_df_mock_y.values)))\n",
    "                    \n",
    "                          \n",
    "            get_family_MSE(test_plate, task_channel, \"Ridge\", \"Mock\", yhat_ridge,std_df_mock_y)\n",
    "            get_family_MAE(test_plate, task_channel, \"Ridge\", \"Mock\", yhat_ridge,std_df_mock_y)\n",
    "            print('**************')\n",
    "            \n",
    "            print('**************')\n",
    "            print('DNN')\n",
    "            print('profile_treated:') \n",
    "            yhat_DNN = pd.DataFrame(DNN_model.predict(std_df_treated_x.values),columns=std_df_treated_y.columns)                           \n",
    "            print('DNN MSE: {}'.format(mean_squared_error(yhat_DNN,std_df_treated_y.values)))  \n",
    "            print('DNN MAE: {}'.format(mean_absolute_error(yhat_DNN,std_df_treated_y.values)))  \n",
    "            #get_family_MSE(yhat_DNN,std_df_treated_y)\n",
    "            #get_family_MAE(yhat_DNN,std_df_treated_y)\n",
    "            print_results(test_plate, task_channel, 'Overall', 'DNN', 'Treated', 'MSE', str(mean_squared_error(yhat_DNN,std_df_treated_y.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'DNN', 'Treated', 'MAE', str(mean_absolute_error(yhat_DNN,std_df_treated_y.values)))\n",
    "                    \n",
    "                          \n",
    "            get_family_MSE(test_plate, task_channel, \"DNN\", \"Treated\", yhat_DNN,std_df_treated_y)\n",
    "            get_family_MAE(test_plate, task_channel, \"DNN\", \"Treated\", yhat_DNN,std_df_treated_y)\n",
    "            \n",
    "            print('profile_mock:')            \n",
    "            yhat_DNN = pd.DataFrame(DNN_model.predict(std_df_mock_x.values),columns=std_df_mock_y.columns)   \n",
    "            print('DNN MSE: {}'.format(mean_squared_error(yhat_DNN,std_df_mock_y.values)))  \n",
    "            print('DNN MAE: {}'.format(mean_absolute_error(yhat_DNN,std_df_mock_y.values)))  \n",
    "                          \n",
    "                          \n",
    "            print_results(test_plate, task_channel, 'Overall', 'DNN', 'Mock', 'MSE', str(mean_squared_error(yhat_DNN,std_df_mock_y.values)))\n",
    "            print_results(test_plate, task_channel, 'Overall', 'DNN', 'Mock', 'MAE', str(mean_absolute_error(yhat_DNN,std_df_mock_y.values)))\n",
    "                    \n",
    "                          \n",
    "            get_family_MSE(test_plate, task_channel, \"DNN\", \"Mock\", yhat_DNN,std_df_mock_y)\n",
    "            get_family_MAE(test_plate, task_channel, \"DNN\", \"Mock\", yhat_DNN,std_df_mock_y)\n",
    "            #get_family_MSE(yhat_DNN,std_df_mock_y)\n",
    "            #get_family_MAE(yhat_DNN,std_df_mock_y)\n",
    "            print('**************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T10:29:16.833379Z",
     "start_time": "2020-08-26T10:13:58.396179Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26569.csv\n",
      "['mock' 'treated']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niko\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\pandas\\core\\frame.py:4170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:03<00:01,  1.69s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.07s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26569.csv\n",
      "AGP:\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "GPU sync failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-048c46a777fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csvs/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Std'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-c559ec8fbc92>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(path, scale_method)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mLR_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_LR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mRidge_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_Ridge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mDNN_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model_dnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask_channel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_train_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_plate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;31m#             svr_model = create_SVR(task_channel,df_train_X, df_train_Y, channel_task_x, channel_task_y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;31m#return task_channel,df_train_X, df_train_Y, channel_task_x, channel_task_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-937c87083332>\u001b[0m in \u001b[0;36mcreate_model_dnn\u001b[1;34m(task_channel, df_train_X, df_train_Y, test_plate)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'{test_plate_number}_{task_channel}.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mmy_callbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_train_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_remove_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \"\"\"\n\u001b[0;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 1008\u001b[1;33m                     signatures, options)\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[0;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[1;32m--> 112\u001b[1;33m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[0;32m    113\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mmodel_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[1;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[0;32m    207\u001b[0m         optimizer_config = {\n\u001b[0;32m    208\u001b[0m             \u001b[1;34m'class_name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             'config': model.optimizer.get_config()}\n\u001b[0m\u001b[0;32m    210\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training_config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizer_config'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\adam.py\u001b[0m in \u001b[0;36mget_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     config.update({\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_serialize_hyperparameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;34m'decay'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_serialize_hyperparameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'decay'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[1;34m'beta_1'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_serialize_hyperparameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'beta_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_serialize_hyperparameter\u001b[1;34m(self, hyperparameter_name)\u001b[0m\n\u001b[0;32m    741\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    744\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   3239\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3240\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3242\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_in_graph_mode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3243\u001b[0m     \u001b[1;31m# This is a variable which was created in an eager context, but is being\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    580\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    584\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m     \"\"\"\n\u001b[0;32m    941\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    908\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PlateDownloader\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: GPU sync failed"
     ]
    }
   ],
   "source": [
    "main('csvs/','Std')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
